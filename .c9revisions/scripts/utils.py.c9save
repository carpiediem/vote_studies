{"ts":1361390050136,"silentsave":true,"restoring":false,"patch":[[{"diffs":[[1,"import os, os.path, errno, sys, traceback\nimport re\nimport logging\nimport htmlentitydefs\nimport json\nimport datetime, time\nimport scrapelib\n\nconfig = None\n\n\n# scraper should be instantiated at class-load time, so that it can rate limit appropriately\nscraper = scrapelib.Scraper(requests_per_minute=120, follow_robots=False, retry_attempts=3)\n\n# Download file at `url`, cache to `destination`. \n# Takes many options to customize behavior.\n\ndef download(url, destination=None, options={}):\n  # uses cache by default, override (True) to ignore\n  force = options.get('force', False)\n\n  # saves in cache dir by default, override (False) to save to exact destination\n  to_cache = options.get('to_cache', True)\n\n  # unescapes HTML encoded characters by default, set this (True) to not do that\n  xml = options.get('xml', False)\n\n  # used by test suite to use special (versioned) test cache dir\n  test = options.get('test', False)\n\n  # if need a POST request with data\n  postdata = options.get('postdata', False)\n\n  if test:\n    cache = test_cache_dir()\n  else:\n    cache = cache_dir()\n\n  if destination:\n    if to_cache:\n      cache_path = os.path.join(cache, destination)\n    else:\n      cache_path = destination\n\n  if destination and (not force) and os.path.exists(cache_path):\n    if not test: logging.info(\"Cached: (%s, %s)\" % (cache, url))\n    with open(cache_path, 'r') as f:\n      body = f.read()\n  else:\n    try:\n      logging.info(\"Downloading: %s\" % url)\n      \n      if postdata:\n        response = scraper.urlopen(url, 'POST', postdata)\n      else:\n        response = scraper.urlopen(url)\n      body = response.bytes # str(...) tries to encode as ASCII the already-decoded unicode content\n    except scrapelib.HTTPError as e:\n      #logging.error(\"Error downloading %s:\\n\\n%s\" % (url, format_exception(e)))\n      return None\n\n    # don't allow 0-byte files\n    if (not body) or (not body.strip()):\n      return None\n\n    # cache content to disk\n    if destination:\n      write(body, cache_path)\n\n  if not xml:\n    body = unescape(body)\n    \n  return body\n\ndef write(content, destination):\n  mkdir_p(os.path.dirname(destination))\n  f = open(destination, 'w')\n  f.write(content)\n  f.close()\n\ndef read(destination):\n  if os.path.exists(destination):\n    with open(destination) as f:\n      return f.read()\n\n# dict1 gets overwritten with anything in dict2\ndef merge(dict1, dict2):\n  return dict(dict1.items() + dict2.items())\n\n# de-dupe a list, taken from:\n# http://stackoverflow.com/questions/480214/how-do-you-remove-duplicates-from-a-list-in-python-whilst-preserving-order\ndef uniq(seq):\n  seen = set()\n  seen_add = seen.add\n  return [ x for x in seq if x not in seen and not seen_add(x)]\n\nimport os, errno\n\n# mkdir -p in python, from:\n# http://stackoverflow.com/questions/600268/mkdir-p-functionality-in-python\ndef mkdir_p(path):\n  try:\n    os.makedirs(path)\n  except OSError as exc: # Python >2.5\n    if exc.errno == errno.EEXIST:\n      pass\n    else: \n      raise\n\ndef xpath_regex(doc, element, pattern):\n  return doc.xpath(\n    \"//%s[re:match(text(), '%s')]\" % (element, pattern), \n    namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\n\n# taken from http://effbot.org/zone/re-sub.htm#unescape-html\ndef unescape(text):\n  def remove_unicode_control(str):\n    remove_re = re.compile(u'[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F]')\n    return remove_re.sub('', str)\n\n  def fixup(m):\n    text = m.group(0)\n    if text[:2] == \"&#\":\n      # character reference\n      try:\n        if text[:3] == \"&#x\":\n          return unichr(int(text[3:-1], 16))\n        else:\n          return unichr(int(text[2:-1]))\n      except ValueError:\n        pass\n    else:\n      # named entity\n      try:\n        text = unichr(htmlentitydefs.name2codepoint[text[1:-1]])\n      except KeyError:\n        pass\n    return text # leave as is\n\n  try:\n    text = re.sub(\"&#?\\w+;\", fixup, text)\n  except:\n    text = text.decode('latin-1')\n    text = re.sub(\"&#?\\w+;\", fixup, text)\n  text = remove_unicode_control(text)\n  return text\n\n# uses config values if present\ndef cache_dir():\n  cache = None\n\n  if config:\n    output = config.get('output', None)\n    if output:\n      cache = output.get('cache', None)\n\n  if not cache:\n    cache = \"cache\"\n\n  return cache\n\ndef test_cache_dir():\n  return \"test/fixtures/cache\"\n\n# uses config values if present\ndef data_dir():\n  data = None\n\n  if config:\n    output = config.get('output', None)\n    if output:\n      data = output.get('data', None)\n\n  if not data:\n    data = \"data\"\n\n  return data"]],"start1":0,"start2":0,"length1":0,"length2":4498}]],"length":4498}
{"contributors":[],"silentsave":true,"ts":1361833094333,"patch":[[{"diffs":[[0," re\n"],[-1,"import logging\n"],[0,"impo"]],"start1":48,"start2":48,"length1":23,"length2":8}]],"length":4483,"saved":false}
{"ts":1361833102367,"patch":[[{"diffs":[[0,"port re\n"],[1,"import logging\n"],[0,"import h"]],"start1":44,"start2":44,"length1":16,"length2":31}]],"length":4498,"saved":false}
